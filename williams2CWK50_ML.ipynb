{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doowilliams/Image-Classification-Using-HOG-Features-and-KNN/blob/main/williams2CWK50_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItJw6H7wClni",
        "outputId": "47b206d8-3142-4388-c16e-d55ee52a9f25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "# Unzip our data\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/dataset/cat_dog1.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "oz6pGZC2bT19"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the image directories\n",
        "image_dir = \"/content/cat_dog1\"\n"
      ],
      "metadata": {
        "id": "q0lyFzDyBzX8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def load_images_and_labels(image_dir, limit=None):\n",
        "    images = []\n",
        "    labels = []\n",
        "    count = 0\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if limit is not None and count >= limit:\n",
        "            break\n",
        "        img = cv2.imread(os.path.join(image_dir, filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "            labels.append(1 if filename.startswith('dog') else 0)  # Assuming dog filenames start with 'dog'\n",
        "            count += 1\n",
        "    return images, labels\n",
        "\n",
        "def bag_of_visual_words(images, n_clusters=100):\n",
        "    sift = cv2.SIFT_create()\n",
        "    descriptors = []\n",
        "    for img in images:\n",
        "        kp, des = sift.detectAndCompute(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), None)\n",
        "        if des is not None:\n",
        "            descriptors.append(des)\n",
        "\n",
        "    descriptors = np.vstack(descriptors)\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, max_iter=100)\n",
        "    kmeans.fit(descriptors)\n",
        "\n",
        "    histograms = []\n",
        "    for des in descriptors:\n",
        "        histogram, _ = np.histogram(kmeans.predict(des.reshape(1, -1)), bins=range(n_clusters+1))\n",
        "        histograms.append(histogram)\n",
        "    histograms = np.vstack(histograms)\n",
        "\n",
        "    combined_histogram = np.sum(histograms, axis=0)  # Combine histograms along the rows\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    normalized_histogram = scaler.fit_transform(combined_histogram.reshape(1, -1))  # Reshape for normalization\n",
        "\n",
        "    return normalized_histogram\n",
        "\n",
        "image_dir = \"/content/cat_dog1\"\n",
        "images, labels = load_images_and_labels(image_dir)\n",
        "\n",
        "# Feature extraction and histogram creation\n",
        "histograms = bag_of_visual_words(images)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(histograms, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=40)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_knn = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (KNN):\", accuracy_knn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "9BebhBEEjDTE",
        "outputId": "2ccd102d-be5b-4f2b-a6a1-3f56c869be86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a081d126706b>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/cat_dog1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Feature extraction and histogram creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a081d126706b>\u001b[0m in \u001b[0;36mload_images_and_labels\u001b[0;34m(image_dir, limit)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Function to load images\n",
        "def loadImages(path, limit=10000):\n",
        "    images = []\n",
        "    classes = []\n",
        "    count = 0\n",
        "    for filename in os.listdir(path):\n",
        "        if count >= limit:\n",
        "            break\n",
        "        category = filename.split('.')[0]\n",
        "        cls = 1 if category == 'dog' else 0\n",
        "        img_path = os.path.join(path, filename)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.resize(image, (128, 128))\n",
        "        images.append(image)\n",
        "        classes.append(cls)\n",
        "        count += 1\n",
        "    return np.array(images), np.array(classes)\n",
        "\n",
        "# Load images and labels\n",
        "images, classes = loadImages(\"/content/cat_dog1\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to extract HOG features\n",
        "def hogFeatures(image):\n",
        "    features = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)\n",
        "    return features\n",
        "\n",
        "hog_features = []\n",
        "for image in X_train:\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    hog_feature = hogFeatures(gray_image)\n",
        "    hog_features.append(hog_feature)\n",
        "\n",
        "# K-means clustering on HOG features to generate visual words vocabulary\n",
        "kmeans = MiniBatchKMeans(n_clusters=100, random_state=42)\n",
        "kmeans.fit(hog_features)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "\n",
        "# Reduce dimensions using PCA\n",
        "pca = PCA(n_components=2)\n",
        "hog_features_pca = pca.fit_transform(hog_features)\n",
        "cluster_labels = kmeans.predict(hog_features)\n",
        "\n",
        "# Plotting the clusters\n",
        "plt.scatter(hog_features_pca[:, 0], hog_features_pca[:, 1], c=cluster_labels, cmap='viridis', marker='.', label='Data Points')\n",
        "plt.title('K-means clustering with PCA')\n",
        "plt.show()\n",
        "\n",
        "# BOW representation\n",
        "x_train_bovw = np.array([np.histogram(kmeans.predict([hogFeatures(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))]), bins=range(101))[0] for image in X_train])\n",
        "x_test_bovw = np.array([np.histogram(kmeans.predict([hogFeatures(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))]), bins=range(101))[0] for image in X_test])\n",
        "\n",
        "# Train KNN classifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=40)\n",
        "classifier.fit(x_train_bovw, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = classifier.predict(x_test_bovw)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_knn = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (KNN):\", accuracy_knn)\n"
      ],
      "metadata": {
        "id": "whGaOuOkkMAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Implementation of Bag of Visual Words-based Approach\n",
        "def bag_of_visual_words(image_dir, n_clusters=50):\n",
        "    # Read images from the directory\n",
        "    images = []\n",
        "    for filename in os.listdir(image_dir):\n",
        "        img = cv2.imread(os.path.join(image_dir, filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "\n",
        "    # Convert images to grayscale\n",
        "    gray_images = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]\n",
        "\n",
        "    # Placeholder for feature extraction\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints = []\n",
        "    descriptors = []\n",
        "    for img in gray_images:\n",
        "        kp, des = sift.detectAndCompute(img, None)\n",
        "        keypoints.append(kp)\n",
        "        descriptors.append(des)\n",
        "\n",
        "    # Stack descriptors\n",
        "    descriptors = np.vstack(descriptors)\n",
        "\n",
        "    # Cluster the features to create visual words\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, max_iter=100)\n",
        "    kmeans.fit(descriptors)\n",
        "    visual_words = kmeans.cluster_centers_\n",
        "\n",
        "    # Build histograms of visual words for each image\n",
        "    histograms = []\n",
        "    for des in descriptors:\n",
        "        histogram, _ = np.histogram(kmeans.predict(des.reshape(1, -1)), bins=range(n_clusters+1))\n",
        "        histograms.append(histogram)\n",
        "    histograms = np.vstack(histograms)\n",
        "\n",
        "    # Normalize the histograms\n",
        "    scaler = StandardScaler()\n",
        "    normalized_histograms = scaler.fit_transform(histograms)\n",
        "\n",
        "    return normalized_histograms\n"
      ],
      "metadata": {
        "id": "w-6DGW5Gxm4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histograms = bag_of_visual_words(image_dir)\n",
        "print(\"Histograms shape:\", histograms.shape)"
      ],
      "metadata": {
        "id": "xMPfO1sBx4ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziCzLCwYKZpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "# Function to resize images\n",
        "def resize_image(img, target_size=(128, 128)):\n",
        "    return cv2.resize(img, target_size)\n",
        "\n",
        "# Implementation of Bag of Visual Words-based Approach\n",
        "def bag_of_visual_words(image_dir, n_clusters=50):\n",
        "    # Read images from the directory\n",
        "    images = []\n",
        "    for filename in os.listdir(image_dir):\n",
        "        img = cv2.imread(os.path.join(image_dir, filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "\n",
        "    # Convert images to grayscale and resize them\n",
        "    gray_images = [resize_image(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)) for img in images]\n",
        "\n",
        "    # Placeholder for HOG feature extraction\n",
        "    hog = cv2.HOGDescriptor()\n",
        "\n",
        "    # Extract HOG features\n",
        "    features = []\n",
        "    for img in gray_images:\n",
        "        try:\n",
        "            feature = hog.compute(img)\n",
        "            if feature is not None and len(feature) > 0:\n",
        "                features.append(feature.flatten())\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image: {e}\")\n",
        "\n",
        "    if len(features) == 0:\n",
        "        raise ValueError(\"No valid HOG features found.\")\n",
        "\n",
        "    features = np.array(features)\n",
        "\n",
        "    # Cluster the features to create visual words\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, max_iter=100)\n",
        "    kmeans.fit(features)\n",
        "    visual_words = kmeans.cluster_centers_\n",
        "\n",
        "    # Build histograms of visual words for each image\n",
        "    histograms = []\n",
        "    for feature in features:\n",
        "        histogram, _ = np.histogram(kmeans.predict(feature.reshape(1, -1)), bins=range(n_clusters+1))\n",
        "        histograms.append(histogram)\n",
        "    histograms = np.vstack(histograms)\n",
        "\n",
        "    # Normalize the histograms\n",
        "    scaler = StandardScaler()\n",
        "    normalized_histograms = scaler.fit_transform(histograms)\n",
        "\n",
        "    return normalized_histograms\n",
        "\n",
        "# Call the function\n",
        "try:\n",
        "    histograms = bag_of_visual_words(image_dir)\n",
        "    print(\"Histograms shape:\", histograms.shape)\n",
        "except ValueError as ve:\n",
        "    print(ve)"
      ],
      "metadata": {
        "id": "b04ZwGVmI_Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "histograms = bag_of_visual_words(image_dir)\n",
        "print(\"Histograms shape:\", histograms.shape)\n",
        "\n",
        "# code for k-NN classification\n",
        "def knn_classification(X_train, y_train, X_test, k=5):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    return knn.predict(X_test)\n",
        "\n",
        "# K-NN usage\n",
        "X_train = histograms\n",
        "y_train = np.random.randint(2, size=len(os.listdir(image_dir)))  # Random labels for demonstration\n",
        "X_test = histograms[:10]  # Example test data\n",
        "predictions = knn_classification(X_train, y_train, X_test)\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "I0g8i9iSJMCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Report: Critique and Recommendation for Petfinder.com's Image Classifier"
      ],
      "metadata": {
        "id": "dKamKxjfJ3-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introductiuon:\n",
        "\n",
        "Petfinder.com aims to develop an efficient binary image classifier capable of distinguishing between images of cats and dogs. The proposed solution involves a supervised learning approach using bags of visual words with k-means clustering of HOG features as features and k-nearest neighbors (KNN) as the classifier. This report critically evaluates this proposal in light of the stakeholders' wishes and makes a final recommendation."
      ],
      "metadata": {
        "id": "TKtOPVTyKH6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stakeholders' Wishes:\n",
        "\n",
        "1. **Lead Data Scientist (S1)**:\n",
        "* Requirement: The system should handle wide\n",
        "variations in imaging conditions.\n",
        "* Analysis: The use of bags of visual words with HOG features may not fully address this requirement. While HOG features are robust to some variations, they may not handle all types of variations efficiently.\n",
        "2. **Lead Software Engineer (S2)**:\n",
        "* Requirement: The system should be implementable in-house with no licensing issues.\n",
        "* Analysis: The proposed solution satisfies this requirement as it uses open-source libraries like OpenCV and scikit-learn.\n",
        "3. **Head of IT (S3)**:\n",
        "* Requirement: The system should be computationally efficient.\n",
        "* Analysis: The proposed solution may have high computational overhead during both training and prediction phases due to feature extraction, clustering, and KNN classification.\n",
        "4. **Lead Data Engineer (S4)**:\n",
        "* Requirement: The online content moderators should have a role after deployment.\n",
        "* Analysis: The proposed solution does not directly address this requirement. Additional features might need to be incorporated to involve online content moderators.\n",
        "5. **Charitable Donor (S5)**:\n",
        "* Requirement: The system should be interpretable.\n",
        "* Analysis: The proposed solution lacks interpretability. KNN as a classifier does not offer insight into decision-making, and bags of visual words may not be easily interpretable."
      ],
      "metadata": {
        "id": "7y02KVoQKaGW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xgKrWaxl2KP"
      },
      "source": [
        "# Critical Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Features**: The use of bags of visual words with HOG features may not adequately capture all variations in imaging conditions. While HOG features are robust to some extent, they may not handle variations like scale, rotation, and backgrounds effectively.\n",
        "* **Classifier**: K-nearest neighbors (KNN) is simple and easy to implement but may not be the best choice for this task. It requires storing all training data, which can be memory-intensive, especially for large datasets like Petfinder's.\n",
        "* **Computational Efficiency**: The proposed solution may have high computational overhead due to feature extraction and clustering. This could lead to scalability issues, especially with millions of images in the dataset.\n",
        "* **Interpretability**: The proposed solution lacks interpretability, which conflicts with the requirement for stakeholders, especially non-technical ones, to understand the system's decisions."
      ],
      "metadata": {
        "id": "V954xBvzLuDu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL0G-h6Omz2U"
      },
      "source": [
        "# Recommendation\n",
        "Based on the critical analysis and stakeholders' wishes, the following recommendations are made:\n",
        "\n",
        "1. **Feature Selection**:\n",
        "Instead of solely relying on HOG features, a more comprehensive feature extraction method like convolutional neural networks (CNNs) should be considered. CNNs can automatically learn relevant features from images, which may better handle variations in imaging conditions.\n",
        "2. **Classifier Selection**:\n",
        "Instead of KNN, a more sophisticated classifier like a support vector machine (SVM) or a deep neural network (DNN) should be explored. These classifiers can provide better performance and scalability.\n",
        "3. **Computational Efficiency**:\n",
        "To address computational overhead, a trade-off between computational complexity and accuracy should be considered. This might involve using a smaller vocabulary size for visual words or optimizing the clustering algorithm.\n",
        "4. **Involvement of Online Content Moderators**:\n",
        "Develop features or functionalities that involve online content moderators in the system, ensuring they have a role in its operation post-deployment.\n",
        "5. **Interpretability**:\n",
        "Incorporate techniques for model interpretability, such as attention mechanisms or feature visualization, to ensure that the system's decisions are understandable to all stakeholders."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "\n",
        "The proposed solution based on bags of visual words with HOG features and KNN classifier has limitations regarding handling imaging variations, computational efficiency, and interpretability. Therefore, it is recommended to explore more advanced techniques such as CNNs for feature extraction and SVMs or DNNs for classification, while also considering the involvement of online content moderators and ensuring interpretability for stakeholders."
      ],
      "metadata": {
        "id": "7eLLPsFfN_Ff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_Sly8FG6SXn"
      },
      "outputs": [],
      "source": [
        "# convert your notebook to html (the file is created in the same directory as this notebook, on your Google Drive)\n",
        "!jupyter nbconvert --to html 2CWK50-ML.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}